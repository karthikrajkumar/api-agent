# ─────────────────────────────────────────────────────────────
# API Agent — Environment Configuration
# ─────────────────────────────────────────────────────────────
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# All variables use the API_AGENT_ prefix OR the short name.
# ─────────────────────────────────────────────────────────────

# ── LLM Provider ────────────────────────────────────────────
# Choose "openai" or "azure"
LLM_PROVIDER=azure

# ── OpenAI (when LLM_PROVIDER=openai) ──────────────────────
#OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_BASE_URL=https://api.openai.com/v1          # default; change for proxies

# ── Azure OpenAI (when LLM_PROVIDER=azure) ─────────────────
AZURE_OPENAI_API_KEY=your-azure-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o
AZURE_OPENAI_API_VERSION=2024-12-01-preview

# ── Model ───────────────────────────────────────────────────
# MODEL_NAME=gpt-5.2                                 # model name (OpenAI) or deployment name (Azure)
# REASONING_EFFORT=                                   # "", "low", "medium", "high"

# ── Server ──────────────────────────────────────────────────
# API_AGENT_HOST=0.0.0.0
# API_AGENT_PORT=3000
# API_AGENT_TRANSPORT=streamable-http                 # "streamable-http", "sse", or "http"
# API_AGENT_CORS_ALLOWED_ORIGINS=*
# API_AGENT_DEBUG=false

# ── MCP Server Identity ────────────────────────────────────
# API_AGENT_MCP_NAME=API Agent
# API_AGENT_SERVICE_NAME=api-agent

# ── Agent Limits ────────────────────────────────────────────
# API_AGENT_MAX_AGENT_TURNS=30                        # max LLM turns per query
# API_AGENT_MAX_RESPONSE_CHARS=50000                  # max chars in final response
# API_AGENT_MAX_SCHEMA_CHARS=32000                    # max schema context for LLM
# API_AGENT_MAX_PREVIEW_ROWS=10                       # rows shown before pagination hint
# API_AGENT_MAX_TOOL_RESPONSE_CHARS=32000             # cap tool responses for LLM context

# ── Polling (async APIs) ───────────────────────────────────
# API_AGENT_MAX_POLLS=20                              # max poll attempts
# API_AGENT_DEFAULT_POLL_DELAY_MS=3000                # ms between polls

# ── Recipe Learning ────────────────────────────────────────
# API_AGENT_ENABLE_RECIPES=true                       # auto-cache successful pipelines
# API_AGENT_RECIPE_CACHE_SIZE=64                      # max recipes per API

# ── OpenTelemetry Tracing (optional) ───────────────────────
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318   # set to enable tracing
